{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"credentials.json\") as f:\n",
    "    credentials = json.loads(f.read())\n",
    "    \n",
    "    host = credentials[\"host\"]\n",
    "    user = credentials[\"db_user\"]\n",
    "    password = credentials[\"db_pass\"]\n",
    "    db = credentials[\"db_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automate this query for every route for every direction\n",
    "\n",
    "engine = create_engine(f\"mysql+pymysql://{user}:{password}@{host}:3306/{db}\")\n",
    "\n",
    "df = pd.read_sql_query('SELECT * FROM trips_2017 WHERE lineid = \"46A\" AND direction = 2', engine)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace missing actual time departure values with planned values\n",
    "\n",
    "df.actualtime_dep.fillna(df.plannedtime_dep, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values for actual time arrival as we cannot safely assume these are as planned\n",
    "\n",
    "df = df[pd.notnull(df['actualtime_arr'])]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for trip duration\n",
    "\n",
    "df['trip_duration'] = df['actualtime_arr'] - df['actualtime_dep']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with the hour of the day the trip took place\n",
    "\n",
    "df['actualtime_dep_H'] = round(df['actualtime_dep']/3600)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hour of day of actual time arrival\n",
    "\n",
    "df['actualtime_arr_H'] = round(df['actualtime_arr']/3600)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average hour of the day of the journey\n",
    "\n",
    "df['avg_H'] = (df['actualtime_dep_H'] + df['actualtime_arr_H']) / 2\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert this to an integer\n",
    "\n",
    "df['avg_H'] = df['avg_H'].astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating column solely for the dates to correlate with the dates column on the historical weather data table\n",
    "\n",
    "df['time'] = df['timestamp'] + df['avg_H'] * 3600\n",
    "df.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing suppressed rows where suppressed = 1.0\n",
    "\n",
    "df = df.query('suppressed != 1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many rows are we working with\n",
    "\n",
    "df.index = range(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating columns from timestamp for further processing\n",
    "\n",
    "df['dayofweek'] = df['timestamp']\n",
    "df['monthofyear'] = df['timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the unix time to datetime format\n",
    "\n",
    "df.dayofweek = pd.to_datetime(df['dayofweek'], unit='s')\n",
    "df.monthofyear = pd.to_datetime(df['monthofyear'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting datetime to name of weekday, and to name of month (in separate columns)\n",
    "\n",
    "df['dayofweek'] = df['dayofweek'].dt.weekday_name\n",
    "df['monthofyear'] = df['monthofyear'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummy variables for weekday names and name of month\n",
    "\n",
    "df_dayofweek_dummies = pd.get_dummies(df['dayofweek'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows not in the month of March\n",
    "# We chose March as we felt it was the best representation of a typical 'school' month\n",
    "\n",
    "df = df.query('monthofyear == 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add day of week columns for each day \n",
    "\n",
    "df1 = pd.concat([df, df_dayofweek_dummies], axis=1, join_axes=[df.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull weather data from database\n",
    "\n",
    "df2 = pd.read_sql_query('SELECT * FROM DarkSky_historical_weather_data WHERE year = 2017 AND month = 3', engine)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace values for clarity purposes i.e. we care if it is cloudy; cloudy-day and cloudy-night distinctions are irrelevant\n",
    "\n",
    "d = {'clear-day':'clear','clear-night':'clear','partly-cloudy-day':'partly-cloudy','partly-cloudy-night':'partly-cloudy'}\n",
    "df2 = df2.replace(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.rename(columns={'day_of_week': 'dayofweek', 'month': 'monthofyear'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mergin bus and weather data on timestamp\n",
    "\n",
    "df3 = pd.merge(df1, df2, on=['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting 'useful' features for analysis\n",
    "\n",
    "df3 = df3[['avg_H', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', 'temp', 'precip_intensity','trip_duration']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trip duration is in seconds, convert to minutes and round to the nearest integer\n",
    "\n",
    "df3['trip_duration'] = round(df3['trip_duration']/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['trip_duration'] = df3['trip_duration'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easier to work with whole number for temperature\n",
    "\n",
    "df3['temp'] = round(df3['temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['temp'] = df3['temp'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Our dataframe is ready for processing\n",
    "\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "You can see that our dataset has eleven columns. The task is to predict the trip duration (last column) based on the day of the week, the time of the day and the weather conditions (temperature and rain intesity). The next step is to split our dataset into attributes and labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign data from first ten columns to X variable\n",
    "# Descriptive features\n",
    "\n",
    "X = df3.iloc[:, 0:10]\n",
    "\n",
    "# Assign data from last column to y variable\n",
    "# Target feature\n",
    "\n",
    "y = df3['trip_duration']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the GBR model\n",
    "\n",
    "# Parameters\n",
    "### n_estimators : int (default=100)\n",
    "    The number of boosting stages to perform. \n",
    "    Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.\n",
    "\n",
    "### max_depth : integer, optional (default=3)\n",
    "    maximum depth of the individual regression estimators. \n",
    "    The maximum depth limits the number of nodes in the tree. \n",
    "    Tune this parameter for best performance; the best value depends on the interaction of the input variables.\n",
    "    \n",
    "### min_samples_split : int, float, optional (default=2)\n",
    "    The minimum number of samples required to split an internal node:\n",
    "    If int, then consider min_samples_split as the minimum number.\n",
    "    If float, then min_samples_split is a percentage and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "        Changed in version 0.18: Added float values for percentages.\n",
    "\n",
    "### learning_rate : float, optional (default=0.1)\n",
    "    learning rate shrinks the contribution of each tree by learning_rate. \n",
    "    There is a trade-off between learning_rate and n_estimators.\n",
    "\n",
    "### loss : {‘deviance’, ‘exponential’}, optional (default=’deviance’)\n",
    "    loss function to be optimized. \n",
    "    ‘deviance’ refers to deviance (= logistic regression) for classification with probabilistic outputs. \n",
    "    For loss ‘exponential’ gradient boosting recovers the AdaBoost algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit regression model\n",
    "# Peter, maybe look at automating trying out a few different parameters and choosing the best one?\n",
    "\n",
    "params = {'n_estimators': 600, 'max_depth': 4, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.02, 'loss': 'ls'}\n",
    "gbr = ensemble.GradientBoostingRegressor(**params)\n",
    "\n",
    "gbr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the importance of each feature based on the model\n",
    "\n",
    "pd.DataFrame({'feature': X.columns, 'importance': gbr.feature_importances_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for the dataset\n",
    "\n",
    "pred = gbr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(pred)\n",
    "predictions.rename(columns={0:'estimated_time'}, inplace=True )\n",
    "predictions['estimated_time'] = round(predictions['estimated_time'])\n",
    "predictions['estimated_time'] = predictions['estimated_time'].astype(int)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the error rate\n",
    "# Peter, if this is more than 9, maybe get the script to flag this pickle, possible with the file name?\n",
    "\n",
    "print(metrics.mean_absolute_error(y,predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the model trained on the full data set to a pickle file\n",
    "# This will need to be automated to contain route and direction\n",
    "\n",
    "pkl_filename = \"GBR_March_2017_46A_2.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:  \n",
    "    pickle.dump(gbr, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
